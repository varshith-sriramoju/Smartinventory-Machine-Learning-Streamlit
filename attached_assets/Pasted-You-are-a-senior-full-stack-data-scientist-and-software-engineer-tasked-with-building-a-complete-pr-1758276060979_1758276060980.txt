You are a senior full-stack data scientist and software engineer tasked with building a complete, production-ready data science application. Your deliverable must be a fully functional, professionally-structured repository that demonstrates enterprise-level software engineering practices. ## Project Specification: SmartInventory - Retail Forecasting Platform Build an end-to-end inventory forecasting system for retail chain management with the following user workflow: Store managers upload historical sales/inventory data via CSV System processes data and trains ML models for demand forecasting Users access dashboards showing current stock levels, predicted stockouts, and reorder recommendations API endpoints serve real-time predictions and batch processing capabilities Automated retraining pipeline maintains model accuracy over time ## Technical Architecture Requirements ### Core Technology Stack (Non-negotiable) Frontend: Responsive HTML5 + modern CSS (Flexbox/Grid) + vanilla JavaScript ES6+ Must be accessible (WCAG 2.1 AA), mobile-responsive, and progressively enhanced Backend: Django (latest stable) + Django REST Framework ML Pipeline: pandas + scikit-learn + joblib for model persistence Include both Jupyter notebooks for exploration AND production scripts Database: PostgreSQL with Django migrations Caching: Redis for prediction caching and rate limiting Task Queue: Celery with Redis broker for async processing Containerization: Docker + docker-compose (local) + Kubernetes manifests (production) CI/CD: GitHub Actions with automated testing, building, and deployment Monitoring: Prometheus metrics + Grafana dashboard + structured JSON logging Documentation: Auto-generated OpenAPI/Swagger UI ### Functional Requirements Breakdown #### 1. Authentication & Authorization JWT-based authentication with three roles: admin, manager, analyst Role-based access control for sensitive operations (retraining, user management) #### 2. Data Management Pipeline CSV upload with client-side validation (file size, format, required columns) Server-side data validation, sanitization, and error handling ETL pipeline generating features: lag variables, rolling averages, seasonality indicators Data versioning and lineage tracking #### 3. Machine Learning Components Model: Gradient boosting regressor for demand forecasting (baseline + optimized versions) Training Pipeline: Exploratory notebook (notebooks/exploration.ipynb) Production training script (scripts/train.py) with CLI arguments Model versioning with metadata (performance metrics, training date, data version) Evaluation: MAE, RMSE, MAPE metrics with automated reporting Serving: Both synchronous API endpoints and asynchronous batch prediction #### 4. API Endpoints (RESTful Design) POST /api/auth/login → JWT token POST /api/data/upload → CSV ingestion job ID GET /api/data/ingestion/{job_id}/status → processing status GET /api/predict → real-time predictions (query params: sku, store_id, date_range) POST /api/predict/batch → async batch prediction job POST /api/models/retrain → trigger model retraining (admin only) GET /api/models → list model versions with performance metrics #### 5. Frontend Dashboard Overview Page: KPI cards (predicted stockouts, at-risk SKUs, inventory turnover) SKU Analysis: Time series charts with Chart.js showing historical and predicted demand Data Upload Interface: Drag-and-drop CSV uploader with progress indicators Admin Panel: Model management, retraining triggers, system health metrics Error Handling: Clear error states, loading spinners, user-friendly messages #### 6. Production Readiness Features Security: Input validation, SQL injection prevention, rate limiting, CORS configuration Performance: Redis caching, database query optimization, async task processing Monitoring: Health check endpoints, Prometheus metrics, structured logging Testing: Unit tests (>80% coverage), integration tests, API contract tests ### Data Schema Specification CSV format with columns: date, store_id, sku_id, sales, price, on_hand, promotions_flag Include synthetic data generator script (scripts/generate_sample_data.py) Provide seed script for demo data population ### Deployment & Infrastructure Local Development: docker-compose up --build starts all services Production: Kubernetes manifests in /k8s/ directory CI/CD: Automated testing, Docker image building, and deployment pipeline Documentation: Architecture diagrams, API specifications, deployment guides ### Quality Assurance Standards Code Quality: PEP8 compliance, type hints, comprehensive docstrings Testing: pytest with fixtures, mocking, and test data factories Documentation: README with clear setup instructions, API documentation, architecture overview Version Control: Semantic commit messages, feature branch workflow ### Deliverable Structure Your repository must contain: ├── README.md (comprehensive setup and architecture guide) ├── docker-compose.yml & Dockerfiles ├── requirements.txt (pinned versions) ├── smartinventory/ (Django project) │   ├── models.py, serializers.py, views.py, tests/ ├── ml/ (machine learning components) │   ├── notebooks/, scripts/train.py, models/ ├── frontend/ (static assets, templates) ├── celery_app/ (async task definitions) ├── k8s/ (Kubernetes manifests) ├── .github/workflows/ci.yml ├── scripts/ (data generation, deployment) └── docs/ (architecture diagrams, API specs) ### Success Criteria Functional: docker-compose up --build → working application at localhost:8000 Complete Workflow: CSV upload → data processing → model training → predictions → dashboard visualization API Compliance: All endpoints documented and testable via Swagger UI at /docs/ Production Ready: CI pipeline passes, containers build successfully, monitoring endpoints active Professional Quality: Clean code, comprehensive tests, clear documentation ### Constraints & Guidelines Must run entirely locally without external cloud dependencies Use only open-source libraries and tools Optimize for maintainability and extensibility Include meaningful code comments explaining design decisions and tradeoffs Follow Django and Python best practices throughout Deliver a repository that demonstrates senior-level software engineering skills with production-quality code, comprehensive testing, and professional documentation. The application should be immediately deployable and maintainable by a development team.